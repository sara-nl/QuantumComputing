{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum machine learning for quantum chemistry problems\n",
    "In this file we will be taking you through the procces of predicting the atomization energy of a molecule using quantum machine learning. To do this we will first introduce you to the data than we will do some classical machine learning to get a benchmark, and lastly we will be using quantum machine learning.\n",
    "\n",
    "Setup of the file:\n",
    "1. Data analysis\n",
    "2. Classical machine learning\n",
    "3. Quantum encodings\n",
    "4. Quantum machine\n",
    "\n",
    "We assume some basic understanding of quantum computing and python programming to read this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pennylane imports\n",
    "import pennylane as qml\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics.pairwise import laplacian_kernel, rbf_kernel, chi2_kernel\n",
    "\n",
    "# Numpy imports\n",
    "import numpy as np\n",
    "\n",
    "# Mathplotlib imports\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Other imports\n",
    "from time import time\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "from matplotlib import cm\n",
    "\n",
    "# Own imports\n",
    "from data import numberOfAtoms, numberOfAtomX\n",
    "from qnn import QNN\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide the atomization energy into equal buckets\n",
    "def devideInBuckets(y, classes):\n",
    "    # The size of a bucket is the total range devded by the amount of classes\n",
    "    stepSize = ((np.amin(y)-np.amax(y))/classes)*-1\n",
    "    print(\"step is:\", stepSize)\n",
    "    \n",
    "    start = np.amax(y)\n",
    "    buckets = np.zeros((y.shape[0], 1))\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        for j in range(classes):\n",
    "            if y[i][0] <= start-(stepSize*j) and y[i][0] >= start-stepSize*(j+1):\n",
    "                    buckets[i][0] = j\n",
    "            else:\n",
    "                continue\n",
    "    return buckets\n",
    "\n",
    "# Return the sum of the coulomb matrix\n",
    "def coulombSum(x):\n",
    "    cm_sum = np.zeros((x.shape[0], 1))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        cm_sum[i][0] = np.sum(x[i])\n",
    "        \n",
    "    return cm_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the qm7b molecular dataset which is a dataset of 7211 molecules in coulomb matrix representation, with 14 molecular properties per coulomb matrix (molecule). A coulomb matrix in way of representing a molecule computationally and its constructed as follows:\n",
    "\n",
    "$$\\begin{split}\n",
    "    C_{ij} &= \\begin{cases}\n",
    "              0.5 Z_{i}^{2.4} & \\text{if } i = j \\\\\n",
    "              & \\\\\n",
    "              \\dfrac{Z_{i}Z_{j}}{\\vert R_{i}-R{j}\\vert} & \\text{if } i \\neq j\n",
    "              \\end{cases},\n",
    "  \\end{split}$$\n",
    "  \n",
    "where $i$ and $j$ are the rows and columns of the Coulomb matrix, $Z$ is the nuclear charge of the atoms ($i$ or $j$), and $R$ is the Cartesian coordinates of the atoms ($i$ or $j$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the qm7b dataset\n",
    "mat = loadmat(file_name=\"qm7b.mat\")\n",
    "\n",
    "# Remove the information in the data set we don't need\n",
    "for key in list(mat.keys()):\n",
    "    if \"__\" in key:\n",
    "        mat.pop(key, None)\n",
    "        \n",
    "# Print the keys\n",
    "print(mat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the coulomb matrix (7211, 23, 23) Y are the properties (7211, 14)\n",
    "X = mat['X']\n",
    "Y = mat['T']\n",
    "\n",
    "# Resize them for sklearn (7211, 529) (7211, 14)\n",
    "x = X.reshape(X.shape[0], -1)\n",
    "y = Y.reshape(Y.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Now that we have the data loaded in we can try and make plot with it to see whats going on. We make a couple of different plots of the atomization energy to get a good picture of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to plot later\n",
    "atomization_energy = np.zeros((y.shape[0], 1))\n",
    "\n",
    "\n",
    "# Isolate the atomization energy which is property 0 of the 14 properties.\n",
    "for i in range(y.shape[0]):\n",
    "    atomization_energy[i][0] = y[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to plot the atomization energy against the coulomb matrix we run into a problem, the coulomb matrix has 529 dimentions. To solve this we can use dimentionallity reducting (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "x1 = pca.fit_transform(x)\n",
    "print(\"Explained variance is: \", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.scatter(x1, atomization_energy, color='blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortionally when trying to reduce the dimention we get a bad explained variance, meaning that our reduced data is not representative of the original data. Because PCA doens't work well in this case, we can plot things like the number of atoms or the sum of the coulomb matrix against the atomization energy to see if there is any relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare array's for different properties.\n",
    "coulomb_sum = np.zeros((x.shape[0], 1))\n",
    "num_atoms = np.zeros((x.shape[0], 1))\n",
    "num_H_atoms = np.zeros((x.shape[0], 1))\n",
    "num_C_atoms = np.zeros((x.shape[0], 1))\n",
    "\n",
    "# Take the sum of he coulomb matrix\n",
    "for i in range(x.shape[0]):\n",
    "    coulomb_sum[i][0] = np.sum(x[i])\n",
    "    \n",
    "# Path variabels in the functions are a problem for later.\n",
    "# These functions find the number of atoms and the number of a specific atom in the molecules.\n",
    "num_atoms = numberOfAtoms(x)\n",
    "num_H_atoms = numberOfAtomX('H')\n",
    "num_C_atoms = numberOfAtomX('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plots\n",
    "fig, axs = plt.subplots(2, 2, )\n",
    "axs[0][0].scatter(coulomb_sum, atomization_energy, color='blue')\n",
    "axs[0][1].scatter(num_atoms, atomization_energy, color='red')\n",
    "axs[1][0].scatter(num_H_atoms, atomization_energy, color='green')\n",
    "axs[1][1].scatter(num_C_atoms, atomization_energy, color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(14,10))\n",
    "scatter = axs.scatter(num_atoms, atomization_energy, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a nice correlation between the number of atoms and the atomization energy in these plots But there is still some overlap if you would try and use this as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine some of the properties into one array\n",
    "a = np.concatenate((coulomb_sum, num_atoms), axis=1)\n",
    "b = np.concatenate((a, num_C_atoms), axis=1)\n",
    "c = np.concatenate((b, num_H_atoms), axis=1)\n",
    "\n",
    "print(c.shape)\n",
    "\n",
    "# PCA to reduce the dimentions\n",
    "pca = PCA(n_components=2)\n",
    "c = pca.fit_transform(c)\n",
    "\n",
    "# Print the shape and the explained variance of the data\n",
    "print(c.shape)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Plot the results of the pcs\n",
    "fig, axs = plt.subplots(1, 1, )\n",
    "scatter = axs.scatter(c[:,0], c[:,1], c=atomization_energy[:,0], cmap=cm.jet_r)\n",
    "colorbar = fig.colorbar(scatter, ax=axs, label = \"Atomization energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(14,10))\n",
    "scatter = axs.scatter(num_H_atoms, num_C_atoms, c=atomization_energy, s=60, edgecolors='black', cmap=cm.jet_r)\n",
    "colorbar = fig.colorbar(scatter, ax=axs, label = \"Atomization energy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "axs = fig.add_subplot(111, projection='3d')\n",
    "scatter = axs.scatter(num_atoms, num_H_atoms, num_C_atoms, c=y[:,0], s=60, edgecolors='black', cmap=cm.jet_r)\n",
    "colorbar = fig.colorbar(scatter, ax=axs, label = \"(kcal/mol)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plots the number of atoms and the number of specific atoms are highly correlated with the atomization energy of the molecule. Altough there is still some overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buckets\n",
    "We try to turn the regression problem into a classification problem by defining \"buckets\" (ranges) of the atomizations energy, and predicting those. We define 5 buckets [2, 3, 6, 8, 10] as that our quantum neural network (QNN) is build to handle those classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of classes\n",
    "n_classes=3\n",
    "\n",
    "# Function to devide the atomization in buckets based on the number of classes you want.\n",
    "buckets = devideInBuckets(atomization_energy, n_classes)\n",
    "\n",
    "print(buckets[0:10])\n",
    "\n",
    "# We plot the ranges\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the split\n",
    "scatter = ax.scatter(num_atoms, atomization_energy, c=buckets, s=60, edgecolors='black', cmap=cm.jet_r)\n",
    "colorbar = fig.colorbar(scatter, ax=ax, label = \"buckets\")\n",
    "\n",
    "# Plot the lines to split the data\n",
    "for u in range(1, n_classes):\n",
    "    plt.hlines(((np.amin(atomization_energy)-np.amax(atomization_energy))/n_classes)*u+np.amax(atomization_energy), 4, 23, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical machine learning\n",
    "For the cassical machine learning part we use sklearn's MLPclassifier to predict the ranges of atomization energy. We start by just using the coulomb matrix to predict the different buckets we established in one part up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the buckets from a collumb vector to a row vector\n",
    "buckets = buckets.reshape(-1)\n",
    "\n",
    "# Split the data into train test splits\n",
    "trainx, testx, trainy, testy = train_test_split(x, buckets, test_size=0.25)\n",
    "print(trainx.shape, testx.shape, trainy.shape, testy.shape)\n",
    "\n",
    "# Train the classifier\n",
    "CLAS = MLPClassifier().fit(trainx, trainy)\n",
    "print(\"----- Class score is:\", CLAS.score(testx, testy), \"Mean absolute error is:\", \n",
    "      mean_absolute_error(testy, CLAS.predict(testx)), \"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already a good result but we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data\n",
    "We can try to use different input data to see wether the score improves. For examples from the plots part we saw that the number of atoms in a molecule is strongly correlated with its atomization energy. So we can try different input data and see how they compare against eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain coulomb matrix\n",
    "CM = x\n",
    "\n",
    "# Sum of the coulomb matrix\n",
    "CMsum = coulombSum(x)\n",
    "\n",
    "# Number of atoms\n",
    "NumberOfAtoms = numberOfAtoms(x)\n",
    "\n",
    "# Number of H atoms\n",
    "NumberOfHAtoms = numberOfAtomX(atom='H')\n",
    "\n",
    "# Number of C atoms\n",
    "NumberOfCAtoms = numberOfAtomX(atom='C')\n",
    "\n",
    "# Sum of coulomb matrix with the number of atoms\n",
    "CM_NumAtoms = np.concatenate((CMsum, NumberOfAtoms), axis=1)\n",
    "\n",
    "# Sum of coulomb matrix with number of atoms and number of H atoms\n",
    "CM_NumAtoms_H = np.concatenate((CM_NumAtoms, NumberOfHAtoms), axis=1)\n",
    "\n",
    "# Sum of coulomb matrix with number of atoms and number of H and C atoms\n",
    "CM_NumAtoms_H_C = np.concatenate((CM_NumAtoms_H, NumberOfCAtoms), axis=1)\n",
    "\n",
    "# PCA on Sum of coulomb matrix with number of atoms and number of H and C atoms\n",
    "# Using pca might helps to perserve the important data\n",
    "P = PCA(n_components=3)\n",
    "PCAall = P.fit_transform(CM_NumAtoms_H_C)\n",
    "print(P.explained_variance_ratio_, np.sum(P.explained_variance_ratio_))\n",
    "\n",
    "inputs = [CM, CMsum, NumberOfAtoms, NumberOfHAtoms, NumberOfCAtoms,  \n",
    "          CM_NumAtoms, CM_NumAtoms_H, CM_NumAtoms_H_C, PCAall]\n",
    "\n",
    "names = [\"CM\", \"CMsum\", \"NumberOfAtoms\", \"NumberOfHAtoms\", \"NumberOfCAtoms\", \"CM_NumAtoms\", \n",
    "         \"CM_NumAtoms_H\", \"CM_NumAtoms_H_C\", \"PCAall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to store the results of the different input data's\n",
    "scores = []\n",
    "mea = []\n",
    "\n",
    "# Compare each input\n",
    "for inp in range(len(inputs)):\n",
    "    # Make training and testing data\n",
    "    trainx, testx, trainy, testy = train_test_split(inputs[inp], buckets, test_size=0.25)\n",
    "    print(names[inp], trainx.shape, testx.shape, trainy.shape, testy.shape)\n",
    "    \n",
    "    # Train the classifier\n",
    "    CLAS = MLPClassifier().fit(trainx, trainy)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"----- Class score is:\", CLAS.score(testx, testy), \"Mean absolute error is:\", \n",
    "          mean_absolute_error(testy, CLAS.predict(testx)), \"-----\")\n",
    "    \n",
    "    # Add the results to the array's\n",
    "    scores.append(CLAS.score(testx, testy))\n",
    "    mea.append(mean_absolute_error(testy, CLAS.predict(testx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores plot\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(names, scores, color='blue')\n",
    "ax.scatter(names, scores, color='red')\n",
    "plt.xticks(rotation=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mae plot\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(names, mea, color='blue')\n",
    "ax.scatter(names, mea, color='red')\n",
    "plt.xticks(rotation=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we can quickly see a few things. Firstly we get the best results with data that involves the number of atoms (noa), as we saw in the plots the noa is very correlated with the atomization energy so this is not suprising. Secondly we see that adding extra information besides just noa can increase the score as seen for the last input. I think this is because adding extra information can negate part of the overlap that you get with only the noa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "Now that we have compared the input data against eachother we can now use a grid search to find good hyperparameters. The gridsearch is just a dictionary of different hyperparameters that are tested agains eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid of different parameter that will be tested against eachother\n",
    "param_grid = {'hidden_layer_sizes': [(5, ), (10, ), (15, ), (20, ), (25, )], \n",
    "              'activation': ['relu', 'logistic', 'tanh', 'identity'], \n",
    "              'solver': ['adam', 'sgd'], \n",
    "              'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "              'learning_rate_init': [0.0001, 0.001, 0.01, 0.1]\n",
    "             }\n",
    "# Train test datasplit\n",
    "trainx, testx, trainy, testy = train_test_split(PCAall, buckets, test_size=0.25)\n",
    "\n",
    "# Classifier\n",
    "Class = MLPClassifier()\n",
    "\n",
    "# The grid search and its best parameters and best score\n",
    "gridsearch = GridSearchCV(CLAS, param_grid, n_jobs=-1).fit(trainx, trainy)\n",
    "print(gridsearch.best_params_)\n",
    "print(gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum encoding\n",
    "There are different methods of encoding classical information into a quantum circuit for this notebook we wil be showing angle, dense angle and amplitude encoding. Our qnn uses dense angle encoding but has the option to use aplitude encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle encoding\n",
    "Angle encoding uses rotations to enode classical information into a quantum circuit. This means N data points can be stored in N qubits, however a advantage of this method is a constant encoding circuit depth as you only need 1 gate per qubit.\n",
    "\n",
    "$$\\vert x\\rangle = \\bigotimes_{i=1}^n \\cos(x_i)|0\\rangle + \\sin(x_i)|1\\rangle .$$\n",
    "\n",
    "\n",
    "### Dense angle encoding\n",
    "Dense angle encoding works the same as angle encoding but it it also adds a phase. because of this data can be 2N data can be stored in N qubits.\n",
    "\n",
    "$$\\vert x\\rangle = \\bigotimes_{i=1}^{\\lceil N/2\\rceil} \\cos(\\pi x_{2i-1})|0\\rangle + e^{2\\pi ix_{2i}}\\sin(\\pi x_{2i-1})|1\\rangle .$$\n",
    "\n",
    "\n",
    "### Amplitude encoding\n",
    "The classical information is encoded in the state vector of the system. This allows you to store N data in log_2(N) qubits, but has a bigger circuit depth compared to the previous methods.\n",
    "\n",
    "$$\\vert x\\rangle = \\frac{1}{||\\textbf{x}||}\\sum_{i=1}^{2^n} x_i |i\\rangle .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum machine learning\n",
    "Now finally we can go and use quantum machine learning. lucky we already have a quantum neural network class (QNN) made. A quantum neural network consists of 4 parts: \n",
    "1. Encoding\n",
    "2. Layer(s) of rotation gates\n",
    "3. Measurement\n",
    "4. Classical optimization\n",
    "\n",
    "![](img/variational.png)\n",
    "\n",
    "The encoding in this case is done using dense angle encoding as discussed above. We use zyz rotation gates combined with CNOT's for the layers. We use pennylane to measure the expectation value and classicly optimize the weights (rotations of the gates) using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This hyperparameter dict is used for the torch optimizer (default: Adam).\n",
    "optdict = {'lr': 0.001}\n",
    "\n",
    "\n",
    "qd = qml.device('default.qubit', shots=1000, wires=PCAall.shape[1])\n",
    "\n",
    "# The qnn splits the data into train and test data.\n",
    "qnn = QNN(x=PCAall, y=buckets, n_layers=1, n_classes=n_classes, reuploading=True, epochs=20, \n",
    "          preperation='angle', qpuRun=False, token=None, batch_size=32, seed=123, quantum_device=qd, **optdict)\n",
    "\n",
    "# True for tracking the accuracy of the network each epoch\n",
    "qnn.train(True)\n",
    "qnn.plotCost()\n",
    "#print(qnn.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
